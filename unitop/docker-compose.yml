version: '3.7'

services:
  test_crawl:
    # image: imagecrawler
    build:
      context: .
      dockerfile: Dockerfile
    container_name: test_crawl
    # volumes:
      # - ./unitop:/usr/src/app
    networks: 
      - mynetwork
    depends_on:
      - mongo

  mongo:
    image: mongo:4.4
    container_name: mongo
    ports:
      - "27020:27017"  # map the MongoDB port to the host
    environment:
      MONGO_INITDB_ROOT_USERNAME: root  # set the root username
      MONGO_INITDB_ROOT_PASSWORD: rootpassword  # set the root password
    volumes:
      - ./mongo-data:/data/db  # persist the database data
    networks:
      - mynetwork
    healthcheck:
      test: ["CMD-SHELL", "mongo --eval 'db.adminCommand(\"ping\")' || exit 1"]  # check MongoDB health by pinging it
      interval: 10s  # how often to run the check (every 10 seconds)
      timeout: 5s    # time to wait for the check to complete (5 seconds)
      retries: 5     # if it fails, how many times to retry
      start_period: 30s  # time to wait before starting health checks (allows MongoDB to initialize)


  spark-master:
    image: bitnami/spark:latest
    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "9090:8080"
      - "7077:7077"
    networks:
      - mynetwork

  spark-worker:
    image: bitnami/spark:latest
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1g
      SPARK_MASTER_URL: spark://spark-master:7077
    networks:
      - mynetwork

  postgres:
    image: postgres
    environment:
      POSTGRES_USER  : admin
      POSTGRES_PASSWORD: admin
    ports:
      - "5432:5432"
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "admin"]
      interval: 5s
      retries: 5
    restart: always
    networks:
      - mynetwork

  pyspark:
    build:
      context: ../spark
      dockerfile: Dockerfile
    container_name: spark-client
    depends_on:
      - spark-master
      - mongo
    environment:
      SPARK_MASTER_URL: spark://spark-master:7077
    volumes:
      - ./spark:/app
    networks:
      - mynetwork
    command: ["spark-submit", "--master", "spark://spark-master:7077", "/app/spark_mongo.py"]


networks:
  mynetwork:
    driver: bridge

volumes:
  mongo-data:
    name: mongo-datalake
  postgres-db-volume: